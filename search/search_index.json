{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DeciBull - Deep Voice Enhancer Working from home!?! Is it hard to understand what your colleagues are saying while working from home? Try DeciBull DeciBull will help improve your online conference experience when talking to your colleagues or friends by improving audio quality on your device.","title":"Home"},{"location":"#decibull-deep-voice-enhancer","text":"","title":"DeciBull - Deep Voice Enhancer"},{"location":"#working-from-home","text":"Is it hard to understand what your colleagues are saying while working from home? Try DeciBull DeciBull will help improve your online conference experience when talking to your colleagues or friends by improving audio quality on your device.","title":"Working from home!?!"},{"location":"About/","text":"About DeciBull DeciBull was developed as a capstone project at UC Berkeley's MIDS program. This application aims to improve your audio experience when talking to your friends or colleagues over any device. The Decibull App seamlessly integrates onto your device and can be added as an extension on your browser or installed via play store. Motivation When audio data is streamed through the internet on platforms such as Zoom, the audio is compressed before transmission. This allows for faster loading times, meaning that you internet does not have to be as fast to receive the information in real time. However, this also means that the quality of the audio is reduced. More specifically, the audio is sampled less frequently (like with lower frame rates on videos), and the highest frequencies of audio are removed altogether. In extreme cases, this can make audio sound like it is underwater. This is where DeciBull can help. DeciBull will take audio data coming in, and transform it using machine learning. Our algorithm will detect where audio information is missing, and work to fill in the gaps. The result is audio with a higher frequency range and sampling rate, which will sound clearer and more crisp to the human ear. DeciBull can upsample audio in real time, so there will be no additional lag when used with live-streamed audio.","title":"About DeciBull"},{"location":"About/#about-decibull","text":"DeciBull was developed as a capstone project at UC Berkeley's MIDS program. This application aims to improve your audio experience when talking to your friends or colleagues over any device. The Decibull App seamlessly integrates onto your device and can be added as an extension on your browser or installed via play store.","title":"About DeciBull"},{"location":"About/#motivation","text":"When audio data is streamed through the internet on platforms such as Zoom, the audio is compressed before transmission. This allows for faster loading times, meaning that you internet does not have to be as fast to receive the information in real time. However, this also means that the quality of the audio is reduced. More specifically, the audio is sampled less frequently (like with lower frame rates on videos), and the highest frequencies of audio are removed altogether. In extreme cases, this can make audio sound like it is underwater. This is where DeciBull can help. DeciBull will take audio data coming in, and transform it using machine learning. Our algorithm will detect where audio information is missing, and work to fill in the gaps. The result is audio with a higher frequency range and sampling rate, which will sound clearer and more crisp to the human ear. DeciBull can upsample audio in real time, so there will be no additional lag when used with live-streamed audio.","title":"Motivation"},{"location":"Contact/","text":"DeciBull","title":"Contact Us"},{"location":"Contact/#decibull","text":"","title":"DeciBull"},{"location":"Examples/","text":"DeciBull","title":"See DeciBull in Action"},{"location":"Examples/#decibull","text":"","title":"DeciBull"},{"location":"How/","text":"How it works DeciBull streams high quality audio i.e. super-resolution audio from the low quality audio transmitted from your device by using General Adversarial networks (GANs). This concept of generating super resolution audio is borrowed from successful use of GANs to generate super resolution images. A GAN comprises of two models i.e. a generator and a discriminator model which are used to resolve missing details in images/audio to generate super resolution images/audio from low quality images. Although our work is heavily inspired from existing research on using GANs to generate super resolution audio, most of the research has been focused on generating batch high resolution audio. DeciBull attempts to use GANs to stream super resolution audio realtime. We use the traditional GAN architecture where the generator comrises of multiple fully connected neural net layers with relu activation function and the discriminator is a simple classification neural network to identify real vs generated high quality audio. DeciBull is trained on more than 80GB of high quality uncompressed speech data from multiple languages so that it can work for different speakers and dialects. We train by compressing our high quality audio data to low quality for training with the target of generating the original high quality audio. Before pushing the low quality audio to the GAN network, we transform the audio waveform into frequency and time domain using a Fourier transform to generate a spectrogram. We then push audio amplitudes at different frequencies for 10ms audio chunks into the model to generate higher quality audio realtime. Applications such as Zoom stream audio at a bit depth of below 64k and Decibull generates audio to match a standardized 320k bit depth and 48k sampling rate. We evaluate our generated audio on two metrics: (1) Square of differences, (2) Bit error rate Our first evaluation metric just evaluates how close the two audio signals (generated vs original high quality audio) are by comparing the raw waveforms at each audio sample and taking the square of differences of the two data points. We also evaluate our audio samples using a more sophisticated methodology by creating an Audio Fingerprint of each audio file. The concept is similar to how apps like Shazam identify songs. Each song has a unique energy sequence at different frequency and time domain and can be given a unique fingerprint using these energy sequence. In our scenario we hash each of these energy values to bits and then compare the overall bit error rate between two audio files. To hash an audio file we convert the audio file into fingerprint blocks and then within each fingerprint block we segment our audio into frequency bands. For each frequency band in a single frame (time window) of audio, we then compute the energy and then hash them into bits by comparing neighbouring energy values. Once we have computed these bits for each frame, we then compare the bits at each frequency and time domain to get the bit errors. Image below shows the audio fingerprints for high quality and low quality files for same audio.","title":"How it works"},{"location":"How/#how-it-works","text":"DeciBull streams high quality audio i.e. super-resolution audio from the low quality audio transmitted from your device by using General Adversarial networks (GANs). This concept of generating super resolution audio is borrowed from successful use of GANs to generate super resolution images. A GAN comprises of two models i.e. a generator and a discriminator model which are used to resolve missing details in images/audio to generate super resolution images/audio from low quality images. Although our work is heavily inspired from existing research on using GANs to generate super resolution audio, most of the research has been focused on generating batch high resolution audio. DeciBull attempts to use GANs to stream super resolution audio realtime. We use the traditional GAN architecture where the generator comrises of multiple fully connected neural net layers with relu activation function and the discriminator is a simple classification neural network to identify real vs generated high quality audio. DeciBull is trained on more than 80GB of high quality uncompressed speech data from multiple languages so that it can work for different speakers and dialects. We train by compressing our high quality audio data to low quality for training with the target of generating the original high quality audio. Before pushing the low quality audio to the GAN network, we transform the audio waveform into frequency and time domain using a Fourier transform to generate a spectrogram. We then push audio amplitudes at different frequencies for 10ms audio chunks into the model to generate higher quality audio realtime. Applications such as Zoom stream audio at a bit depth of below 64k and Decibull generates audio to match a standardized 320k bit depth and 48k sampling rate. We evaluate our generated audio on two metrics: (1) Square of differences, (2) Bit error rate Our first evaluation metric just evaluates how close the two audio signals (generated vs original high quality audio) are by comparing the raw waveforms at each audio sample and taking the square of differences of the two data points. We also evaluate our audio samples using a more sophisticated methodology by creating an Audio Fingerprint of each audio file. The concept is similar to how apps like Shazam identify songs. Each song has a unique energy sequence at different frequency and time domain and can be given a unique fingerprint using these energy sequence. In our scenario we hash each of these energy values to bits and then compare the overall bit error rate between two audio files. To hash an audio file we convert the audio file into fingerprint blocks and then within each fingerprint block we segment our audio into frequency bands. For each frequency band in a single frame (time window) of audio, we then compute the energy and then hash them into bits by comparing neighbouring energy values. Once we have computed these bits for each frame, we then compare the bits at each frequency and time domain to get the bit errors. Image below shows the audio fingerprints for high quality and low quality files for same audio.","title":"How it works"},{"location":"Team/","text":"Our Team Below: Tyler Ryu (Data Scientist) Below: Patricia Degner (Data Scientist) Below: Suyash Dusad (Data Scientist) Suyash Dusad (Data Scientist)","title":"Team"},{"location":"Team/#our-team","text":"Below: Tyler Ryu (Data Scientist) Below: Patricia Degner (Data Scientist) Below: Suyash Dusad (Data Scientist) Suyash Dusad (Data Scientist)","title":"Our Team"}]}