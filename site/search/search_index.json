{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"DeciBull DeciBull is a neural network based audio upsampling product which up-samples low quality audio to higher quality to improve user experience on online calling platforms. Experience it now Download on MacOS Download on Windows","title":"DeciBull"},{"location":"#decibull","text":"","title":"DeciBull"},{"location":"About/","text":"About DeciBull DeciBull was developed as a capstone project at UC Berkeley's MIDS program. This application aims to improve your audio experience when talking to your friends or colleagues over any device. The Decibull App seamlessly integrates onto your device by changing your devices output to the DeciBull extension output on your browser.","title":"About DeciBull"},{"location":"About/#about-decibull","text":"DeciBull was developed as a capstone project at UC Berkeley's MIDS program. This application aims to improve your audio experience when talking to your friends or colleagues over any device. The Decibull App seamlessly integrates onto your device by changing your devices output to the DeciBull extension output on your browser.","title":"About DeciBull"},{"location":"AudioSuperResolution/","text":"Super-Resolution Audio DeciBull streams high quality audio i.e. super-resolution audio from the low quality audio transmitted from your device by using generative models such as General Adversarial networks (GANs)/Auto Encoders. This concept of generating super resolution audio is borrowed from successful use of GANs and AutoEncoders to generate super resolution images. A GAN comprises of two models i.e. a generator and a discriminator model which are used to resolve missing details in images/audio to generate super resolution images/audio from low quality images. Although our work is heavily inspired from existing research on using GANs to generate super resolution audio, most of the research has been focused on generating batch high resolution audio. DeciBull attempts to use generative models to stream super resolution audio realtime. Super-resolution Audio generation using GANs","title":"Super-Resolution Audio"},{"location":"AudioSuperResolution/#super-resolution-audio","text":"DeciBull streams high quality audio i.e. super-resolution audio from the low quality audio transmitted from your device by using generative models such as General Adversarial networks (GANs)/Auto Encoders. This concept of generating super resolution audio is borrowed from successful use of GANs and AutoEncoders to generate super resolution images. A GAN comprises of two models i.e. a generator and a discriminator model which are used to resolve missing details in images/audio to generate super resolution images/audio from low quality images. Although our work is heavily inspired from existing research on using GANs to generate super resolution audio, most of the research has been focused on generating batch high resolution audio. DeciBull attempts to use generative models to stream super resolution audio realtime. Super-resolution Audio generation using GANs","title":"Super-Resolution Audio"},{"location":"Contact/","text":"Contact Us For any questions and clarifications please write in to us. Tyler Ryu - tylerkryu@berkeley.edu Patricia Degner - pdegner@berkeley.edu Suyash Dusad - suyash18@berkeley.edu","title":"Contact Us"},{"location":"Contact/#contact-us","text":"For any questions and clarifications please write in to us. Tyler Ryu - tylerkryu@berkeley.edu Patricia Degner - pdegner@berkeley.edu Suyash Dusad - suyash18@berkeley.edu","title":"Contact Us"},{"location":"Evaluation/","text":"Audio Quality Evaluation Evaluation of audio quality is very important to understand how good or bad our generated audio is. Human ears are very sharp in identifying any background noise or disturbance in audio quality. However for DeciBull we wanted to comeup with more automated metrics that can help us evaluate our model output. We evaluate our generated audio on two metrics: (1) Square of differences (2) Bit error rate Our first evaluation metric just evaluates how close the two audio signals (generated vs original high quality audio) are by comparing the raw waveforms at each audio sample and taking the square of differences of the two data points. Comparing square differences of two audio waveforms We also evaluate our audio samples using a more sophisticated methodology by creating an Audio Fingerprint of each audio file. The concept is similar to how apps like Shazam identify songs. Each song has a unique energy sequence at different frequency and time domain and can be given a unique fingerprint using these energy sequence. In our scenario we hash each of these energy values to bits and then compare the overall bit error rate between two audio files. To hash an audio file we convert the audio file into fingerprint blocks and then within each fingerprint block we segment our audio into frequency bands. For each frequency band in a single frame (time window) of audio, we then compute the energy and then hash them into bits by comparing neighbouring energy values. Once we have computed these bits for each frame, we then compare the bits at each frequency and time domain to get the bit errors. Image below shows the audio fingerprints for high quality and low quality files for same audio. Comparing fingerprints for low vs high quality audio Our model is integrated with an electron GUI that can be integrated with your audio device by changing the output of audio. This will stream your low quality audio to high quality generated audio to improve your online calling experience.","title":"Audio Quality Evaluation"},{"location":"Evaluation/#audio-quality-evaluation","text":"Evaluation of audio quality is very important to understand how good or bad our generated audio is. Human ears are very sharp in identifying any background noise or disturbance in audio quality. However for DeciBull we wanted to comeup with more automated metrics that can help us evaluate our model output. We evaluate our generated audio on two metrics: (1) Square of differences (2) Bit error rate Our first evaluation metric just evaluates how close the two audio signals (generated vs original high quality audio) are by comparing the raw waveforms at each audio sample and taking the square of differences of the two data points. Comparing square differences of two audio waveforms We also evaluate our audio samples using a more sophisticated methodology by creating an Audio Fingerprint of each audio file. The concept is similar to how apps like Shazam identify songs. Each song has a unique energy sequence at different frequency and time domain and can be given a unique fingerprint using these energy sequence. In our scenario we hash each of these energy values to bits and then compare the overall bit error rate between two audio files. To hash an audio file we convert the audio file into fingerprint blocks and then within each fingerprint block we segment our audio into frequency bands. For each frequency band in a single frame (time window) of audio, we then compute the energy and then hash them into bits by comparing neighbouring energy values. Once we have computed these bits for each frame, we then compare the bits at each frequency and time domain to get the bit errors. Image below shows the audio fingerprints for high quality and low quality files for same audio. Comparing fingerprints for low vs high quality audio Our model is integrated with an electron GUI that can be integrated with your audio device by changing the output of audio. This will stream your low quality audio to high quality generated audio to improve your online calling experience.","title":"Audio Quality Evaluation"},{"location":"Examples/","text":"DeciBull Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. High Quality (320Kbps) Compressed (8Kbps) Reconstructed (320Kbps) Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. High Quality (320Kbps) Compressed (8Kbps) Reconstructed (320Kbps)","title":"DeciBull"},{"location":"Examples/#decibull","text":"Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. High Quality (320Kbps) Compressed (8Kbps) Reconstructed (320Kbps) Your browser does not support the audio element. Your browser does not support the audio element. Your browser does not support the audio element. High Quality (320Kbps) Compressed (8Kbps) Reconstructed (320Kbps)","title":"DeciBull"},{"location":"GAN/","text":"DeciBull Network Architecture We use the a generative model architecture where the generator comprises of multiple fully connected neural net layers with relu activation function and instead of using a discriminator we optimise the generator loss by comparing the real vs generated high quality audio. DeciBull is trained on more than 80GB of high quality uncompressed speech data from multiple languages so that it can work for different speakers and dialects. Network architecture for DeciBull We train by compressing our high quality audio data to low quality for training with the target of generating the original high quality audio. Before pushing the low quality audio to the network, we transform the audio waveform into frequency and time domain using a Fourier transform to generate a spectrogram. We then push audio amplitudes at different frequencies for 10ms audio chunks into the model to generate higher quality audio realtime. Applications such as Zoom stream audio at a bit depth of below 64k and Decibull generates audio to match a standardized 320k bit depth and 48k sampling rate.","title":"DeciBull Network Architecture"},{"location":"GAN/#decibull-network-architecture","text":"We use the a generative model architecture where the generator comprises of multiple fully connected neural net layers with relu activation function and instead of using a discriminator we optimise the generator loss by comparing the real vs generated high quality audio. DeciBull is trained on more than 80GB of high quality uncompressed speech data from multiple languages so that it can work for different speakers and dialects. Network architecture for DeciBull We train by compressing our high quality audio data to low quality for training with the target of generating the original high quality audio. Before pushing the low quality audio to the network, we transform the audio waveform into frequency and time domain using a Fourier transform to generate a spectrogram. We then push audio amplitudes at different frequencies for 10ms audio chunks into the model to generate higher quality audio realtime. Applications such as Zoom stream audio at a bit depth of below 64k and Decibull generates audio to match a standardized 320k bit depth and 48k sampling rate.","title":"DeciBull Network Architecture"},{"location":"Motivation/","text":"Motivation When audio data is streamed through the internet on platforms such as Zoom, the audio is compressed before transmission. This allows for faster loading times, meaning that you internet does not have to be as fast to receive the information in real time. However, this also means that the quality of the audio is reduced. More specifically, the audio is sampled less frequently (like with lower frame rates on videos), and the highest frequencies of audio are removed altogether. In extreme cases, this can make audio sound like it is underwater. This is where DeciBull can help. DeciBull will take audio data coming in, and transform it using machine learning. Our algorithm will detect where audio information is missing, and work to fill in the gaps. The result is audio with a higher frequency range and sampling rate, which will sound clearer and more crisp to the human ear. DeciBull can upsample audio in real time, so there will be no additional lag when used with live-streamed audio.","title":"Motivation"},{"location":"Motivation/#motivation","text":"When audio data is streamed through the internet on platforms such as Zoom, the audio is compressed before transmission. This allows for faster loading times, meaning that you internet does not have to be as fast to receive the information in real time. However, this also means that the quality of the audio is reduced. More specifically, the audio is sampled less frequently (like with lower frame rates on videos), and the highest frequencies of audio are removed altogether. In extreme cases, this can make audio sound like it is underwater. This is where DeciBull can help. DeciBull will take audio data coming in, and transform it using machine learning. Our algorithm will detect where audio information is missing, and work to fill in the gaps. The result is audio with a higher frequency range and sampling rate, which will sound clearer and more crisp to the human ear. DeciBull can upsample audio in real time, so there will be no additional lag when used with live-streamed audio.","title":"Motivation"},{"location":"Team/","text":"Our Team Tyler Ryu (Data Scientist) Patricia Degner (Data Scientist) Suyash Dusad (Data Scientist)","title":"Our Team"},{"location":"Team/#our-team","text":"Tyler Ryu (Data Scientist) Patricia Degner (Data Scientist) Suyash Dusad (Data Scientist)","title":"Our Team"},{"location":"Why/","text":"Why you need DeciBull? Is it hard to understand what your colleagues are saying while working from home? DeciBull will help enhance your experience when talking to your colleagues or friends by transmitting higher quality audio on your device.","title":"Why you need DeciBull?"},{"location":"Why/#why-you-need-decibull","text":"","title":"Why you need DeciBull?"}]}